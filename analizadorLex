# Importamos la libreria
import ply.lex as lex
# Creamos la lista de tokens
# Agregamos en un diccionario las palabras reservadas del lenguaje
reservadas = {
              'class'   :   'CLASS',
              'return'  :   'RETURN',
              'this'    :   'THIS',
              'extends' :   'EXTENDS',
              'if'      :   'IF',
              'new'     :   'NEW',
              'void'    :   'VOID',
              'else'    :   'ELSE',
              'length'  :   'LENGTH',
              'int'     :   'INT',
              'while'   :   'WHILE',
              'true'    :   'TRUE',
              'boolean' :   'BOLEAN',
              'break'   :   'BREAK',
              'false'   :   'FALSE',
              'string'  :   'STRING',
              'continue'    :   'CONTINUE',
              'null'    :   'NULL',    
              }

#Definimos los tipos y operadores del lenguaje como tokens
tokens = [
#Operadores Logicos de comparacion
   'OR',                ## ||
   'AND',               ## &&
   'NOT',               ## NOT
   'MENOR',             ## <
   'MENORIGUAL',        ## <=
   'MAYOR',             ## >
   'MAYORIGUAL',        ## >=
   'IGUALIGUAL',        ## ==
   'DIFERENTE',         ## !=

# Operadores Aritmeticos
   'SUMA',              ## +
   'RESTA',             ## -
   'MULT',              ## *
   'DIV',               ## /
   'MODULO',            ## %
   'IGUAL',             ## =

# Simbolos Especiales
   'LLIZQ',             ## {
   'LLDER',             ## }
   'PARIZQ',            ## (
   'PARDER',            ## )
   'CORIZQ',            ## [           
   'CORDER',            ## ]
   
# Tipos Numericos
    'HEXA',             ## 0x...
    'FLOAT',            ## 0.00...     
    'ENTERO',           ## 0           

# Signos de Puntuacion
    'COMA',             ## ,
    'PUNTO',            ## . 
    'PUNTOCOMA',        ## ;
    'DOSPUNTOS',        ## :
    
# Literales
    'COMENTARIO',       ## /**/ - // //
    'IDENTIFICADOR',    ## Tipo
    'CADENA'            ## "Esto es una cadena"
    
] + list(reservadas.values())

# Los literales son todas expresiones atomicas que nuestro lenguaje necesite para realizar una operacion
literales = ('=','<','>','.',';',':',',','-','+','/','*','(',')','[',']','{','}','!','%')


# Definicion de las reglas de las Expresiones Regulares A Los Tokens
# Operadores Logicos de comparacion
t_OR = r'\|\|'
t_AND = r'&&'
t_NOT = r'~'
t_MENOR = r'<'
t_MENORIGUAL = r'<='
t_MAYOR = r'>'
t_MAYORIGUAL = r'>='
t_DIFERENTE = r'!='
t_IGUALIGUAL = r'=='

# Operadores Aritmeticos
t_SUMA = r'\+'
t_RESTA = r'-'
t_MULT = r'\*'
t_DIV = r'/'
t_MODULO = r'%'
t_IGUAL = r'='

# Simbolor Especiales
t_LLIZQ = r'\{'
t_LLDER = r'\}'
t_PARDER  = r'\('
t_PARIZQ  = r'\)'
t_CORIZQ = r'\['
t_CORDER = r'\]'

# Signos De Puntuacion
t_COMA = r','
t_PUNTO = r'\.'
t_PUNTOCOMA = r';'
t_DOSPUNTOS = r':'

############################################################################
################ Token Para Los Diferentes Errores #########################
############################################################################

# Token para el manejo de errores
def t_error(t):
    print "Caracter ilegal '%s'" % t.value[0]
    t.lexer.skip(1)

# Token para el manejo de error del comentario no abierto    
def t_error_COMENTARIO_NoAbierto(t): 
        r'\*/'
        print "Error en COMENTARIO, en la linea '%d': No abierto"%(t.lexer.lineno)
        pass

# Token para el manejo de error del comentario no cerrado
def t_error_COMENTARIO_NoCerrado(t): 
        r'/\*.*(?!\*/)'
        print "Error en COMENTARIO, en la linea '%d': No cerrado"%(t.lexer.lineno)
        pass       

# Token para errores presentes en numeros de puntos flotantes
def t_ERROR_puntodec(t):
        #r'[0-9]*[\.][\.]+[0-9]+(?:[eE][-+][0-9]+)?'
        r'((\d+[\.][\.]+\d*|\d*[\.][\.]+\d+)([eE][-+]?[0-9]+)?)'
        print "Error en Notacion, en la linea '%d' mas de un [\.] "%(t.lexer.lineno)

def t_ERROR_notacientifica(t):
        r'[-+]?[0-9]*\.[0-9]+[eE][eE]+[-+]?[0-9]+'
        print "Error en Notacion, en la linea '%d' mas de un simbolo e a la izquierda"%(t.lexer.lineno)

    
##############################################################################

# Expresion Regular Para Los Numeros En Flotante
def t_FLOAT(t):
    r'(([-+]?\d+\.\d*|\d*\.\d+)([eE][-+]?[0-9]+)?)|[-+]?([0-9]+[eE][-+]?[0-9]+)'
    t.value = float(t.value)
    return t

# Expresion Regular Para Los Numeros En Hexa 
def t_HEXA(t):
    r'0[xX][0-9A-F]+'
    return t

# Expresion Regular Para Los Numeros En Entero
def t_ENTERO(t):
    r'-?\d+|0'
    t.value = int(t.value)
    return t

# Expresion Regular Para El Comentario @ /* jkxGZKxgzkx */
def t_COMENTARIOMULTIPLE(t):
    r'/\*(.|\n)*?\*/'
    t.lexer.lineno += t.value.count('\n')

# Expresion Regular Para El Comentario @ //
def t_COMENTARIOSIMPLE(t):
    r'//.*\n'
    t.lexer.lineno += 1

# Expresion Regular Para Los Strings
def t_STRING(t):
    r'".*"'
    return t

# Token para los identificadores que NO pueden ser a su vez palabras reservadas
def t_IDENTIFICADOR(t):
    r'[a-zA-Z_][a-zA-Z0-9_]*'
    t.type = reservadas.get(t.value,'IDENTIFICADOR') 
    if len(t.value)>20:
        print "El valor de ID excede  en la linea %d posicion %d en identificador: (%s): excede los caracteres permitidos" % (t.lexer.lineno,t.lexer.lexpos,t.value)
        l = t.value
        print l[:20]
        t.value = l[:20]
        return t
    else:
        return t 

# Token de contador de saltos de linea
def t_nuevalinea(t):
    r'\n+'
    t.lexer.lineno += len(t.value)
    
# Token para ignorar caracteres especiales
t_ignore  = ' \t'

#########################################################################
####### Construccion, Recorrido Del Archivo Y Funcionamiento ############
#########################################################################

# Leemos el codigo fuente del archivo
# Que se pasara a una al "lexer" para leer de manera secuencial caracter a caracter
# Recordando que todo el archivo lo lee como texto plano.

# Abrimos el archivo en modo lectura
archivo = open('/home/serna/Compiladores /Lexer/pruebas/pruebaid.txt','r')
# Guardamos el archivo en una variable codigo
codigo = archivo.read()
# Cerramos el archivo
archivo.close()
# Creamos el lexer
lexer = lex.lex()
# Pasamos el codigo fuente al lexer como entrada  
lexer.input(codigo)
# Creamos un loop para que empieze a reconocer los tokens hasta que no alla un siguiente
while True:
    tok = lexer.token()
    if not tok:break
    print tok
